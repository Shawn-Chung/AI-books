#周志华《机器学习》——学习笔记

##第一章 概述

 * 若我们预测的是**离散值**，这种学习任务称为**分类**。
 * 若我们预测的是**连续值**，这种学习任务称为**回归**。

 * 只涉及两个类别的为**二分类**：其中一个叫*正类*，一个叫*反类*。
 * 涉及多个类别的为**多分类**。

##第二章 模型评估与选择

 * **错误率或误差**——分类错误的样本数占样本总数的比例，E = a/m。
 * **精度**——1-错误率。
 * **训练误差、经验误差**——学习器在训练集上的误差。
 * **泛化误差**——学习器在新样本上的误差。
 * **过拟合（overfitting）**——我们训练的目标是让学习器在新样本上的误差也就是泛化误差尽可能的小。但当学习器对训练样本学习得“太好了”，把训练样本自身的一些特点当做了所有潜在样本都具有的一般性质，就会导致泛化性能的下降。**（过拟合是机器学习面临的关键障碍，各类算法都必须带有一些针对过拟合的措施，但是过拟合是无法彻底避免的，能做的只是缓解、减小其风险）**

![过拟合-欠拟合](./photo/过拟合-欠拟合.png)

 * **欠拟合**——对训练样本的一般性质都尚未学好，泛化能力变弱。

### 模型评估方法 ###
	
通常，我们可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此须使用一个**测试集**来测试学习器对新样本的判别能力，然后以测试集上的“测试误差”作为泛化误差的近似。

如何从样本集中分别产生出训练集和测试集，有以下几种方法：

 * **留出法**：从样本集中划分一部分作为测试集，常见做法是将大约 2/3~4/5的样本用于训练，剩余样本用于测试。
 * **交叉验证法**：先将数据集划分为 k 个大小相似的互斥子集，然后每次使用 k-1 个子集的并集作为训练集余下的那个子集作为测试集，这样可以获得 k 组训练/测试集，k 通常取值为10.

![过拟合-欠拟合](./photo/交叉验证.png)

 * **自助法**：从样本集中随机选出 m 个样本作为训练集，其中 m 中可能有部分样本是重复的，也就是说从 样本集 D 中选取时，同一个样本可能被多次选中。

 * **调参**：超参数的设置。

## 第三章 线性模型 ##

 * 基本形式：
><a href="http://www.codecogs.com/eqnedit.php?latex=f(x)=w_{1}x_{1}&plus;w_{2}x_{2}&plus;...&plus;w_{d}x_{d}&plus;b" target="_blank"><img src="http://latex.codecogs.com/gif.latex?f(x)=w_{1}x_{1}&plus;w_{2}x_{2}&plus;...&plus;w_{d}x_{d}&plus;b" title="f(x)=w_{1}x_{1}+w_{2}x_{2}+...+w_{d}x_{d}+b" /></a>

 * 向量形式：
><a href="http://www.codecogs.com/eqnedit.php?latex=f(x)=w^{T}x&plus;b" target="_blank"><img src="http://latex.codecogs.com/gif.latex?f(x)=w^{T}x&plus;b" title="f(x)=w^{T}x+b" /></a>

### 线性回归 ###

><a href="http://www.codecogs.com/eqnedit.php?latex=D&space;=&space;\left&space;\{&space;(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})&space;\right.\left.\right&space;\}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?D&space;=&space;\left&space;\{&space;(x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m})&space;\right.\left.\right&space;\}" title="D = \left \{ (x_{1},y_{1}),(x_{2},y_{2}),...,(x_{m},y_{m}) \right.\left.\right \}" /></a>其中
><a href="http://www.codecogs.com/eqnedit.php?latex=x_{i}&space;=&space;\left&space;(&space;x_{i1};x_{i1};...;x_{i1}&space;\right&space;),y_{i}\in&space;R" target="_blank"><img src="http://latex.codecogs.com/gif.latex?x_{i}&space;=&space;\left&space;(&space;x_{i1};x_{i1};...;x_{i1}&space;\right&space;),y_{i}\in&space;R" title="x_{i} = \left ( x_{i1};x_{i1};...;x_{i1} \right ),y_{i}\in R" /></a>

线性回归试图学得一个线性模型预测输入值在模型的左右下其输出值逼近标签值。即学习一个模型：
><a href="http://www.codecogs.com/eqnedit.php?latex=f(x)=wx_{i}&plus;b" target="_blank"><img src="http://latex.codecogs.com/gif.latex?f(x)=wx_{i}&plus;b" title="f(x)=wx_{i}+b" /></a>     

使得
><a href="http://www.codecogs.com/eqnedit.php?latex=f(x_{i})\simeq&space;y_{i}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?f(x_{i})\simeq&space;y_{i}" title="f(x_{i})\simeq y_{i}" /></a>

其关键问题就是如何确定 w 和 b？ 核心就是如何衡量 f(x) 与 y 之间的差别。常用的方法是**均方误差**。其对应了**欧几里得距离或者“欧氏距离”**。而基于均方误差最小化来进行模型求解的方法称为**最小二乘法**。在线性回归中，最小二乘法就是试图找到一条直线，试所有样本到直线上的欧式距离之和最小。

### 对数几率回归（logistic regression） ###

上面讨论的线性回归其预测值：
><a href="http://www.codecogs.com/eqnedit.php?latex=z&space;=&space;w^{T}x&plus;b" target="_blank"><img src="http://latex.codecogs.com/gif.latex?z&space;=&space;w^{T}x&plus;b" title="z = w^{T}x+b" /></a>

是**实值**，而考虑到**二分类**任务，其输出标记是：
><a href="http://www.codecogs.com/eqnedit.php?latex=y&space;\subset&space;\left&space;\{&space;\left.&space;0,1&space;\right&space;\}&space;\right." target="_blank"><img src="http://latex.codecogs.com/gif.latex?y&space;\subset&space;\left&space;\{&space;\left.&space;0,1&space;\right&space;\}&space;\right." title="y \subset \left \{ \left. 0,1 \right \} \right." /></a>

于是，我们需将实值 z 转化为 0/1 值，最理想的就是**单位阶跃函数**，但是单位阶跃函数不连续，不能求导。于是就有了一定程度上近似单位阶跃函数的近似函数，它单调可微。就是**对数几率函数（logistic）**即**Sigmoid函数**：
><a href="http://www.codecogs.com/eqnedit.php?latex=y&space;=&space;\frac{1}{1&plus;e^{-z}}" target="_blank"><img src="http://latex.codecogs.com/gif.latex?y&space;=&space;\frac{1}{1&plus;e^{-z}}" title="y = \frac{1}{1+e^{-z}}" /></a>

该函数将输入实数值压缩到0，1之间。

Sigmoid函数带入 z 即可得：
><a href="http://www.codecogs.com/eqnedit.php?latex=ln\frac{y}{1-y}&space;=&space;w^{T}x&plus;b" target="_blank"><img src="http://latex.codecogs.com/gif.latex?ln\frac{y}{1-y}&space;=&space;w^{T}x&plus;b" title="ln\frac{y}{1-y} = w^{T}x+b" /></a>

若将 y 看着样本 x 最为正例的可能性，则 1-y 是其反例的可能性，两者的比值即为**几率**，反映了 x 作为正例的相对可能性，对几率取对数则得到**对数几率**。

综上，这种方法实际上是利用线性回归模型的预测结果去逼近真实标记的对数几率，因此这个模型称为**对数几率回归**，也就是**逻辑回归**，注意，这里虽然名字叫回归，其实是一种分类学习方法。
逻辑回归不仅预测出“类别”，而是可得到近似的概率预测，这对许多需要利用概率辅助决策的任务很有用。

## 第四章 决策树 ##
